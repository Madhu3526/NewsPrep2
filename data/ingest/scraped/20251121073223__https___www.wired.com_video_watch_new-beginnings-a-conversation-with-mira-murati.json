{
  "title": "New Beginnings: A Conversation with Mira Murati",
  "text": "Transcript\n\n[audience clapping]\n\n[energetic music]\n\nAre y'all having a good time?\n\n[Audience Member] Oh, yeah. [audience cheering]\n\nYeah. Okay, great,\n\nwell, I'm excited about our next guest\n\nin the big interview, Mira Murati.\n\nShe recently, most recently served\n\nas Chief Technology Officer at OpenAI,\n\nwhere among other things,\n\nshe oversaw product teams building ChatGPT, DALL·E, Sora,\n\nand contributed to advancements in ai safety, ethics,\n\nand machine learning research.\n\nFor a few days, she was even the interim CEO, I here.\n\n[audience laughing]\n\nShe also handled the external relationships.\n\nAnd I could tell you, I did a story on Microsoft recently\n\nand unprompted,\n\nany number of people told me how important she was\n\nto the partnership\n\nand how they enjoyed working with her\n\nand how she made things go better for\n\nthat little company in the Northwest.\n\nPrior to joining opening OpenAI, she managed the product\n\nand engineering teams at Leap Motion\n\nand led the design, development\n\nand launch of vehicle products at Tesla,\n\nincluding the Model X.\n\nIs that all correct?\n\nYeah.\n\nWell, welcome to Mira.\n\nThank you for having us.\n\n[audience clapping]\n\nOkay, Mira, in September,\n\nyou left OpenAI with a very generous and diplomatic note.\n\nYou say you were going to do your own exploration.\n\nNow I've been reading unconfirmed reports\n\nthat you've been fundraising\n\nand you might be working\n\nwith some other people from OpenAI.\n\nNow, here's a spoiler alert.\n\nI know from our prep you're not gonna be talking a lot about\n\nwhat you're doing in terms of that,\n\nbut what can you tell us?\n\nCan you tell us anything about what you're up to next?\n\nI'm not going to share much about what I'm doing next\n\nbecause I am figuring out what that looks like.\n\nI'm in the midst of it,\n\nbut I can tell you a bit about what I'm excited\n\n[Steven] Okay.\n\nIn the future.\n\n[Steven] Sure.\n\nAnd yeah, generally I would totally ignore the noise\n\nand the speculation externally.\n\nI think actually there is just too much noise\n\nand obsession about who is leaving the labs and so on\n\nand let's focus on the actual substance of things.\n\nBut what I'm excited about is, you know, quite similar\n\nto the set of things that I was working on earlier,\n\nbut perhaps from a slightly different angle.\n\nI think I'm very optimistic about the future.\n\nI think we are about to see immense potential\n\nwith abundance of energy and intelligence\n\nand even meaning.\n\nAnd I really think that we're sort of at this beginning\n\nof infinite age of curiosity and wonder\n\nand deepening our knowledge about the world.\n\nAnd the hardest part is gonna be figuring out\n\nhow our civilization co-evolve with the development\n\nof science and technology as our knowledge deepens.\n\nAnd I hope that I can continue\n\nto contribute positively in that direction.\n\nSo your optimism,\n\nI think is something I saw not only with you,\n\nbut other folks at OpenAI.\n\nIt was a company that where people had a shared vision,\n\nI think you might even referred to it\n\nas something spiritual at one point.\n\nAnd it was centered about belief that, you know,\n\nhumanity really was on this quest to take, you know,\n\nwhat was suddenly possible to do\n\nand developing digital technology\n\nto do human like and beyond performance\n\nand things, you know, something called AGI,\n\nwhat some people called it.\n\nAnd that was within our grasp.\n\nYou did share that view\n\nand I guess do share\n\nthat we're approaching the point\n\nwhere we can accomplish that, is that correct?\n\nYeah, but, you know,\n\njust to kind of state the assumptions\n\nof what it actually means,\n\nI'll define AGI as sort of,\n\nyou know, a system that is capable\n\nof learning how to perform at human level\n\nacross all cognitive tasks.\n\nAnd you know, if you look at sort\n\nof the past few years this year,\n\nwe have systems that are capable\n\nof PhD level performance.\n\n[Steven] Yeah.\n\nIn many different domains\n\nand before that we had college level performance.\n\nAnd before that, just a couple of years\n\nbefore that we had high school level performance.\n\nSo if you just look at this trend\n\nand extrapolate it out, you know,\n\nit's not for certain,\n\nbut it shows that progress will likely continue.\n\nAnd it's not unreasonable\n\nthat in a very short time we could get to a system\n\nthat has a capability to learn how to perform at human level\n\nacross this basically all cognitive tasks.\n\nAnd right now this feels, I would say,\n\nquite achievable, even if it doesn't take, you know,\n\neven if it's not something that happens within a couple\n\nof years, it'll take perhaps a decade,\n\nmaybe two, I don't know,\n\nbut it feels achievable.\n\nWhereas I'd say, you know, even six years ago,\n\nto me it felt more sci-fi.\n\nAnd even though I was inspired\n\nand we believed in this, what you call spiritual mission\n\nand common vision,\n\nit felt quite sci-fi at the time.\n\nWhereas now we've made enough progress that we can kind\n\nof see how the technology evolves.\n\nWhen did you first begin to understand\n\nthat this was possible?\n\nHow did you, you know,\n\nbecome involved in working with AI?\n\nWas it at Tesla earlier in your education?\n\nYeah, so sort of by background,\n\nI was very drawn to math and sciences early on,\n\nand I went on to study mechanical engineering,\n\nworked in aerospace as an engineering,\n\nand kind of got a sense of how to build\n\nand develop complex systems\n\nin the world given real constraints and, you know,\n\nsystems that are safety critical.\n\nBut it was a Tesla where I got intuitive feel\n\nfor how AI would really advanced transportation.\n\nAnd there I started to think more about\n\nhow it would affect other domains\n\nand particularly how it would change our relationship\n\nto knowledge and information.\n\nAnd this is when I got interested\n\nin exploring virtual reality, augmented reality,\n\nand from the lens\n\nof exploring the human-machine interface.\n\nAnd while I was doing that, I was actually reading\n\nquite a bit of Vernor Vinge,\n\nand it was his essay on Singularity where he talks about,\n\nyou know, this is sort of likens our era\n\nto a time where, where the change is so transformational\n\nand it's quite similar to the rise\n\nof human life on earth.\n\n[Steven] Right.\n\nAnd it felt quite sci-fi,\n\nbut at the same time, it was enough,\n\nit was grounded enough on real possibility.\n\nAnd to me it felt like even if there was, you know,\n\n2% chance of this being possible,\n\nit would be the most important thing that I would do.\n\nAnd OpenAI mission really resonated with me to,\n\nyou know, ensure that AGI would benefit humanity.\n\nYou know, you mentioned, you know, the VR company you work\n\nfor, it seems, you know, like an interesting\n\non your resume, like almost like a side trip there.\n\nI mean, if this interview were taking place\n\nor if this conference were taking place like six years ago,\n\nall people would be talking about was the metaverse, right,\n\nand I don't think any\n\nof the sessions here are about the metaverse, you know,\n\nwe're, we're talking about AI.\n\nIs this something you still believe in?\n\nYou know, that, you know, AR and VR\n\nwill be super important technologies?\n\nYeah, my approach to VR\n\nand AR wasn't so much from a perspective\n\nthat I thought it would happen in that particular time.\n\nI was more curious\n\nto understand this next human-machine interface\n\nand what that could look like.\n\nI think virtual reality\n\nand augmented reality are have definitely advanced a lot,\n\nsince then.\n\nAnd yeah, I think we will definitely see great technologies,\n\nbut we will also see other interfaces.\n\nYou know, in talking about hype, some people feel\n\nthat AI has been crazily hyped\n\nand they're somehow saying that at this moment, you know,\n\nit shows that, oh, it's slowing down,\n\nit's plateauing is what they're saying.\n\nAnd they're saying\n\nthat the next generation models\n\nare not going to be the kind of leap we saw\n\nfrom like, the generation of GPT-3 to GPT-4,\n\nwhich was, you know, kind of like an astounding leap.\n\nDo you push back against that?\n\nDo you think AI is plateauing?\n\nSo I think one interesting observation is that people get,\n\nthey adapt very quickly to these changes.\n\nLike, you know, the ChatGPT and Claude\n\nand all the systems that we have today\n\nthat maybe they think they're not good enough,\n\nthey're not proving fast enough.\n\nSo I'll make that observation\n\nand maybe that is good signal for what's about\n\nto come in our society's ability to adapt to more change.\n\nBut in terms of whether there is a plateau\n\nor not, let's consider where the progress came from.\n\nAnd a lot of the progress today has come from, you know,\n\nincreasing the size of the neural networks,\n\nincreasing the amount of data, increasing the amount\n\nof compute that goes into the systems.\n\nAnd we've observed this scaling law,\n\nwhich is not literally a law,\n\nbut an observation rather,\n\nthat as you increase all of these things predictably leads\n\nit to increased capability.\n\nAnd in 2020 we saw this with text,\n\nbut since then we've seen it with a lot\n\nof different data code and images and video and so on.\n\nSo we've seen a lot of advancement coming from that.\n\nAnd then another vector has been of progress\n\nhas been multimodality.\n\nAnd there is also, you know,\n\nwe're just starting to see the rise of more agentic systems.\n\nSo I expect there is going to be a lot of progress there.\n\nBut then the question is, will this progress,\n\nwill this scaling laws lead us to systems\n\nthat are capable of performing a human level\n\nacross all cognitive tasks?\n\nI would say that, you know,\n\ncurrent evidence shows that the progress\n\nwill likely continue.\n\nAnd I don't think there is a lot of evidence\n\nto the contrary,\n\nbut whether we need, you know, new ideas to get\n\nto AGI level systems or not, that that's uncertain.\n\nAnd also it's very possible\n\nthat we hit limitations in architectures\n\nor, you know, other methods.\n\nBut then when that happens,\n\nit turns out that--\n\n[Steven] Yeah,\n\nPeople will find a way around it\n\nand there'll be new techniques,\n\nand new optimizations.\n\nSo I would say it's uncertain,\n\nbut I, I'm quite optimistic that the progress will continue.\n\nThere are also, I'd say, yeah, the counter arguments\n\nthat I've heard are, you know, the data wall\n\nand sort of the compute investment.\n\nAnd on the data wall,\n\npeople are exploring things like synthetic data\n\nwhere models generate their own data.\n\nAnd on compute, if we look at the level\n\nof investment on compute, you know,\n\nthis year companies are spending a billion dollars\n\nand next year that goes up by a factor of 10 to 10 billion\n\nand the year after that to a hundred billion.\n\nSo from capability perspective,\n\nit seems like progress will continue.\n\nBut I think getting\n\nto AGI level systems is not just about capability\n\nand it's also about figuring out\n\nhow we make the systems aligned, safe.\n\nIt's about figuring out the entire social infrastructure\n\nin which these systems are going to be operated in.\n\n[Steven] Right,\n\nSo that we can have a positive future\n\nbecause this technology is not intrinsically good or bad,\n\nit comes with both sides.\n\nSo you mentioned safety.\n\nYou know, I have to say when I was diving\n\ninto OpenAI, you know,\n\nwhich was founded to build AGI safely,\n\nthe people I talked to in general,\n\nI'm not talking about you necessarily.\n\nthey got more excited about the building AGI part\n\nthan the safety part.\n\nNot that they dismissed it,\n\nbut that's what really got their motor running.\n\nYou know, they'd light up\n\nwhen they talk about about that.\n\nDo you feel that we're spending enough attention\n\nto the safety and, you know,\n\nbecause there's kind of an arm race going on,\n\nlike kind of, there's literally, you know,\n\na race to best, you know,\n\nthis company the best than another,\n\nyour model's better than my model, you know.\n\nI've gotta fix that and race ahead.\n\nDo you think we're overrunning safety,\n\nyou think we're paying enough attention?\n\nI think that on practical safety,\n\nwe've actually made a lot of progress.\n\nThe work that OpenAI has done\n\non practical alignment has been incredible\n\nand it has really led the industry.\n\nAnd that's been very interesting to see\n\nbecause it is also,\n\nI think kind of like the market dynamics have pushed\n\neveryone in the industry to really innovate in that vector.\n\nBut there is a lot of work\n\non more theoretical alignment\n\nand that I think we're lacking,\n\nbut not only also sort of unlike governance.\n\nWhat what does it mean to live in a world\n\nwith this AGI level systems?\n\nAnd also I think regulation is lagging\n\nbehind basically the entire infrastructure.\n\nI would say that civilization needs to coexist harmoniously\n\nwith this technology\n\nis really lagging behind\n\nWhat worries you more,\n\nthe sort of problems we might have\n\nand we're actually seeing with misinformation\n\nor, you know, bias,\n\nyou know, and things like that.\n\nOr the, you know,\n\nlonger term as existential kind of threats\n\nthat people paint AI have.\n\nSome people have said that the as existential threats\n\nare brought up as sort of\n\nlike a distraction\n\nbecause people aren't really building the safety stuff now,\n\nyou know, which worries you more?\n\nI'd say both, but more perhaps on the longer\n\nterm safety questions,\n\nbecause I think that there is\n\nmarket alignment on the short term safety questions\n\naround misinformation and bias\n\nbecause, you know, it's not good for business\n\nto have AI systems integrated in your business\n\nthat are making things up.\n\nAnd so I think a lot of effort will actually is already\n\nand will continue to go into this set of problems.\n\nBut one area that's lacking that's more near term\n\nis sort of the transparency and the AI literacy.\n\nA lot of people, a majority\n\nof the world doesn't have a good understanding\n\nof what's going on.\n\nThese systems are black boxes.\n\nAnd I think investing more\n\nin the understanding of what these systems are capable of,\n\nhow they work, how we control them,\n\ninvesting more in the direction,\n\ngiving people an intuition for, you know,\n\nwhere they have control and where they don't\n\nand also what what we expect in the future.\n\nI think those things are very important.\n\nOkay, can you explain to me,\n\nyou've been working on these products\n\nwhy we haven't been able to get rid of the Hallucinations?\n\nSo one way to think about it is,\n\nyeah, actually Von Neumann wrote an essay in 1955\n\nwhere he talks about sort of the harmful\n\nand positive aspects of a technology\n\nand how they're always tied together.\n\n[Steven] Yeah.\n\nAnd he says, you know, I will just paraphrase\n\nsomething like, These things are always tied together\n\nand you almost cannot distinguish,\n\nit's impossible to distinguish the lion from the lamb.\n\nAnd I think hallucinations are like that where it gives you,\n\nyou know, this ability for the model\n\nto provide very imaginative outputs,\n\nbut at the same time in a different context\n\nthat can be quite damaging\n\nand harmful if you're operating in a context\n\nwhere you need very accurate information in, you know,\n\nlegal context or medicine or so on.\n\nAnd, you know, since the development\n\nand deployment of the LMS in the real world,\n\nwe've developed new techniques like using tools,\n\nusing search, getting citation and so on.\n\nBut it's still something that we need to figure out.\n\nYou know, that brings up\n\nsort of an interesting question.\n\nI know, you know, OpenAI is struggling, indeed,\n\nit's being sued for, you know, IP,\n\nthe allegedly and the training sets.\n\nSome people have suggested, you know, you talked earlier\n\nof synthetic like data\n\nthat that could be one way to get around that.\n\nBut it seems to me that the more we go down this path,\n\nthe more valuable, the trustworthy information is,\n\nyou know, like made by humans.\n\nThere was a study I read about recently\n\nthat talked about that if models are trained on, you know,\n\ndata which is produced by AI, it sort of, you know,\n\ngets awful results\n\nand you know, in each iteration it kind of goes\n\nto meaninglessness, right,\n\nwhich seems to put a premium on like human created\n\ncontent to put it in the training sets.\n\nSo inevitably what happens?\n\nYou know, it winds up to be some sort of licensing things\n\nfor the best, most trustworthy models,\n\nwhich then sort of, I guess limits its world models.\n\nHow are we going to eventually deal with this IP issue\n\nand have reliable, world knowledgeable models?\n\nYeah, I think that's probably the answer to that is\n\nprobably very nuance.\n\nThere is the aspect of, you know, how the laws evolve\n\nwith the dawn of this technology\n\nand there's a question of that.\n\nThere is another question of how do you make sure\n\nthat the people that have contributed data\n\nalong the way are, are part of, you know,\n\nsomehow are--\n\nYeah.\n\nTaking part into benefits\n\nand figuring out and innovating perhaps in business models\n\nand understanding, doing more research\n\nand understanding how specific data contribution\n\nleads to the model providing, you know,\n\na certain amount of revenue.\n\nAnd another layer is definitely the research on the data\n\nand figuring out what kind of data you can use\n\nand pushing areas like post training more,\n\nwhich is, you know, you're using techniques\n\nlike our reinforcement learning with human feedback\n\nor you're doing reinforcement learning from AI feedback,\n\nlike the constitutional AI stuff,\n\nor you're using other techniques.\n\nBut I think this is one area actually\n\nthat is getting more and more sophisticated\n\nin the modern AI systems\n\nand requires a lot of human feedback or synthetic data.\n\nYou know, finally,\n\nI guess what happens if we do get AGI?\n\nYour former boss says is gonna be an age\n\nof unbelievable abundance.\n\nYou know, the poorest person in the future will be,\n\nwell better off than the richest people now.\n\nYou must have given us a lot of thought.\n\nWhere are we gonna be if we get this AGI,\n\nwhich can match and exceed some human capabilities\n\nand then learn to go beyond us,\n\nwhat's that world look like?\n\nI think that depends on us.\n\nWe have a lot of agency\n\nfor how things are about to evolve\n\nand how civilization co-evolves with this technology.\n\nI think it's entirely up to us, the institutions,\n\nthe structures we put into place, the level of investment,\n\nthe work that we do,\n\nand really how we move forward the entire ecosystem.\n\nI think right now there is a lot of focus\n\non specific individuals,\n\nbut the real question there is\n\nhow do all of us contribute\n\nto this ecosystem to move it forward in a way\n\nthat's collectively positive\n\nand that is really what will shape the actions\n\nand constrain the actions of any specific individuals.\n\nAnd I hope that more people focus on that,\n\nthis is not going to be up to a single company\n\nor individual to bring AGI to the entire civilization.\n\nWell thank you very much, this is great.\n\nThank you.\n\n[audience clapping]",
  "authors": [
    "Condé Nast"
  ],
  "url": "https://www.wired.com/video/watch/new-beginnings-a-conversation-with-mira-murati"
}