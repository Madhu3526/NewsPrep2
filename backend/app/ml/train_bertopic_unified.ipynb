{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1821a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded: 20000\n",
      "Loading SBERT embedder: all-mpnet-base-v2\n",
      "Building UMAP model...\n",
      "Building HDBSCAN model...\n",
      "Initializing BERTopic...\n",
      "Training BERTopic model on 20000 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 13:48:41,890 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b55b38ceced443dbc76c0d6d119c297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 14:04:43,837 - BERTopic - Embedding - Completed âœ“\n",
      "2025-11-21 14:04:43,890 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-21 14:05:30,075 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-11-21 14:05:30,079 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-21 14:06:27,790 - BERTopic - Cluster - Completed âœ“\n",
      "2025-11-21 14:06:27,828 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-21 14:06:31,315 - BERTopic - Representation - Completed âœ“\n",
      "2025-11-21 14:06:34,725 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Saving BERTopic model to: D:\\newsprep2\\backend\\app\\ml\\models\\topics\\bertopic_unified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 14:06:53,599 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-11-21 14:06:53,649 - BERTopic - Representation - Fine-tuning topics using representation models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers Rate (-1): 0.2276\n",
      "Sample topic info:\n",
      "    Topic  Count                                Name  \\\n",
      "0     -1   4553                    -1_the_to_of_and   \n",
      "1      0    560       0_season_quarterback_ap_coach   \n",
      "2      1    519  1_bush_kerry_convention_john kerry   \n",
      "3      2    514             2_najaf_sadr_iraq_iraqi   \n",
      "4      3    244           3_fund_mutual_enron_funds   \n",
      "\n",
      "                                      Representation  \\\n",
      "0      [the, to, of, and, in, for, on, with, 39, is]   \n",
      "1  [season, quarterback, ap, coach, ap ap, opener...   \n",
      "2  [bush, kerry, convention, john kerry, presiden...   \n",
      "3  [najaf, sadr, iraq, iraqi, cleric, shrine, al ...   \n",
      "4  [fund, mutual, enron, funds, securities, shell...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [Stocks Fall on Security, Earnings Worries  NE...  \n",
      "1  [Warner Snaps Back As rookie Eli Manning strug...  \n",
      "2  [Anti-Kerry Ads Not Unfair, Laura Bush Says (R...  \n",
      "3  [Militants Remove Arms From Najaf Shrine NAJAF...  \n",
      "4  [Two Investment Banks Settle with SEC (Reuters...  \n",
      "\n",
      "Reducing topics to 50 (optional)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 14:06:56,972 - BERTopic - Representation - Completed âœ“\n",
      "2025-11-21 14:06:56,982 - BERTopic - Topic reduction - Reduced number of topics from 293 to 50\n",
      "2025-11-21 14:06:58,162 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced model saved at: D:\\newsprep2\\backend\\app\\ml\\models\\topics\\bertopic_unified_reduced\n",
      "\n",
      "ðŸŽ‰ BERTopic training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# train_bertopic_unified.py\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "import hdbscan\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# ============================\n",
    "# Paths\n",
    "# ============================\n",
    "CORPUS_CSV = r\"D:\\newsprep2\\data\\merged\\unified_corpus.csv\"\n",
    "MODELS_DIR = r\"D:\\newsprep2\\backend\\app\\ml\\models\\topics\"\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# ============================\n",
    "# Load Data\n",
    "# ============================\n",
    "df = pd.read_csv(CORPUS_CSV)\n",
    "\n",
    "# SAMPLE for faster first-time testing\n",
    "docs = df[\"text\"].astype(str).tolist()\n",
    "docs = docs[:20000]   # keep sampling for now\n",
    "\n",
    "print(\"Documents loaded:\", len(docs))\n",
    "\n",
    "# ============================\n",
    "# Embedding Model\n",
    "# ============================\n",
    "EMBEDDER = \"all-mpnet-base-v2\"   # strong but slower\n",
    "# EMBEDDER = \"all-MiniLM-L6-v2\"  # faster for CPU\n",
    "\n",
    "print(f\"Loading SBERT embedder: {EMBEDDER}\")\n",
    "sbert = SentenceTransformer(EMBEDDER)\n",
    "\n",
    "# ============================\n",
    "# UMAP + HDBSCAN (legacy compatible)\n",
    "# ============================\n",
    "print(\"Building UMAP model...\")\n",
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,\n",
    "    metric=\"cosine\",\n",
    "    low_memory=True\n",
    ")\n",
    "\n",
    "print(\"Building HDBSCAN model...\")\n",
    "hdbscan_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=20,\n",
    "    min_samples=5,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# BERTopic Model\n",
    "# ============================\n",
    "print(\"Initializing BERTopic...\")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=sbert,\n",
    "    n_gram_range=(1, 2),\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Train Model\n",
    "# ============================\n",
    "print(\"Training BERTopic model on\", len(docs), \"documents...\")\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# ============================\n",
    "# Save Model\n",
    "# ============================\n",
    "save_path = os.path.join(MODELS_DIR, \"bertopic_unified\")\n",
    "print(\"Saving BERTopic model to:\", save_path)\n",
    "\n",
    "topic_model.save(save_path)\n",
    "\n",
    "# ============================\n",
    "# Topic Info\n",
    "# ============================\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "topic_info.to_csv(os.path.join(MODELS_DIR, \"bertopic_info.csv\"), index=False)\n",
    "topic_info.to_json(\n",
    "    os.path.join(MODELS_DIR, \"bertopic_info.json\"),\n",
    "    orient=\"records\",\n",
    "    force_ascii=False\n",
    ")\n",
    "\n",
    "# Save topic keywords\n",
    "keywords = {}\n",
    "for t in topic_info.Topic.tolist():\n",
    "    if t == -1:\n",
    "        continue\n",
    "    keywords[int(t)] = [w for w, _ in topic_model.get_topic(t)]\n",
    "\n",
    "with open(os.path.join(MODELS_DIR, \"bertopic_keywords.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ============================\n",
    "# Diagnostics\n",
    "# ============================\n",
    "frac_outliers = (pd.Series(topics) == -1).mean()\n",
    "print(f\"Outliers Rate (-1): {frac_outliers:.4f}\")\n",
    "print(\"Sample topic info:\\n\", topic_info.head())\n",
    "\n",
    "# ============================\n",
    "# Optional: Reduce Topics\n",
    "# ============================\n",
    "print(\"\\nReducing topics to 50 (optional)...\")\n",
    "reduced_model = topic_model.reduce_topics(docs, nr_topics=50)\n",
    "reduced_save_path = os.path.join(MODELS_DIR, \"bertopic_unified_reduced\")\n",
    "reduced_model.save(reduced_save_path)\n",
    "\n",
    "print(\"Reduced model saved at:\", reduced_save_path)\n",
    "print(\"\\nðŸŽ‰ BERTopic training completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import os\n",
    "\n",
    "MODELS_DIR = r\"D:\\newsprep2\\backend\\app\\ml\\models\\topics\"\n",
    "topic_model = BERTopic.load(os.path.join(MODELS_DIR, \"bertopic_unified\"))\n",
    "\n",
    "# Visualize topics (interactive)\n",
    "fig = topic_model.visualize_topics()\n",
    "fig.show()   # in notebook, this will open interactive plot\n",
    "\n",
    "# Visualize topic probabilities for a single topic id\n",
    "fig2 = topic_model.visualize_barchart(top_n_topics=20)\n",
    "fig2.show()\n",
    "\n",
    "# Visualize topic similarity (map)\n",
    "topic_model.visualize_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d86661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign_topics_to_corpus.py\n",
    "import os, pandas as pd\n",
    "from bertopic import BERTopic\n",
    "\n",
    "CSV = r\"D:\\newsprep2\\data\\merged\\unified_corpus.csv\"\n",
    "\n",
    "MODELS_DIR = r\"D:\\newsprep2\\backend\\app\\ml\\models\\topics\"\n",
    "OUT_CSV = r\"D:\\newsprep2\\data\\merged/unified_corpus_with_topics.csv\"\n",
    "\n",
    "topic_model = BERTopic.load(os.path.join(MODELS_DIR, \"bertopic_unified\"))\n",
    "\n",
    "df = pd.read_csv(CSV)\n",
    "docs = df[\"text\"].astype(str).tolist()\n",
    "\n",
    "# Transform in batches to avoid memory issues\n",
    "batch_size = 5000\n",
    "topic_ids = []\n",
    "topic_probs = []\n",
    "for i in range(0, len(docs), batch_size):\n",
    "    batch = docs[i:i+batch_size]\n",
    "    t, p = topic_model.transform(batch)\n",
    "    topic_ids.extend([int(x) for x in t])\n",
    "    # p may be None if probabilities not available for some models; handle\n",
    "    if p is None:\n",
    "        topic_probs.extend([None]*len(t))\n",
    "    else:\n",
    "        # p is array of probabilities per doc (float)\n",
    "        # if p is an array of arrays, take first item\n",
    "        try:\n",
    "            topic_probs.extend([float(x) for x in p])\n",
    "        except Exception:\n",
    "            topic_probs.extend([None]*len(t))\n",
    "\n",
    "df[\"bertopic_topic\"] = topic_ids\n",
    "df[\"bertopic_topic_prob\"] = topic_probs\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved:\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c128f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_lda.py\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaModel\n",
    "from ml.data_preprocessing import clean_text   # if running from ml/ adjust path\n",
    "\n",
    "CORPUS_CSV = \"data/merged/unified_corpus.csv\"\n",
    "OUT_DIR = \"backend/app/ml/models/topics/lda\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(CORPUS_CSV)\n",
    "# Use the cleaned text if present, else preprocess raw\n",
    "if \"clean_text\" in df.columns:\n",
    "    texts = df[\"clean_text\"].astype(str).tolist()\n",
    "else:\n",
    "    texts = [clean_text(t).split() for t in df[\"text\"].astype(str).tolist()]\n",
    "# If clean_text exists it's a string; split into tokens:\n",
    "if isinstance(texts[0], str):\n",
    "    texts = [t.split() for t in texts]\n",
    "\n",
    "# Create Dictionary & Corpus\n",
    "id2word = corpora.Dictionary(texts)\n",
    "id2word.filter_extremes(no_below=10, no_above=0.5, keep_n=50000)\n",
    "corpus = [id2word.doc2bow(t) for t in texts]\n",
    "\n",
    "# Train LDA\n",
    "NUM_TOPICS = 20   # tune\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=NUM_TOPICS, random_state=42, passes=10, chunksize=2000)\n",
    "\n",
    "# Save model & topics\n",
    "lda_model.save(os.path.join(OUT_DIR, \"lda.model\"))\n",
    "id2word.save(os.path.join(OUT_DIR, \"id2word.dict\"))\n",
    "\n",
    "lda_topics = {i: [word for word,_ in lda_model.show_topic(i, topn=10)] for i in range(NUM_TOPICS)}\n",
    "with open(os.path.join(OUT_DIR, \"lda_topics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(lda_topics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"LDA topics saved to\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1fcbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.display(vis)   # in notebook\n",
    "# Or save html:\n",
    "pyLDAvis.save_html(vis, \"backend/app/ml/models/topics/lda/lda_vis.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_topic_labels.py\n",
    "import json\n",
    "from bertopic import BERTopic\n",
    "MODELS_DIR = \"backend/app/ml/models/topics\"\n",
    "topic_model = BERTopic.load(os.path.join(MODELS_DIR, \"bertopic_unified\"))\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Auto-generate short labels using top keywords\n",
    "labels = {}\n",
    "for t in topic_info.Topic:\n",
    "    if t == -1:\n",
    "        labels[-1] = \"No clear topic\"\n",
    "        continue\n",
    "    words = topic_model.get_topic(t)\n",
    "    label = \" / \".join([w for w,_ in words[:4]])\n",
    "    labels[int(t)] = label\n",
    "\n",
    "with open(os.path.join(MODELS_DIR, \"bertopic_labels.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labels, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved topic labels:\", os.path.join(MODELS_DIR, \"bertopic_labels.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9105d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fallback_nearest_topics.py\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "MODELS_DIR = \"backend/app/ml/models/topics\"\n",
    "topic_model = BERTopic.load(os.path.join(MODELS_DIR, \"bertopic_unified\"))\n",
    "sbert = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# ensure topic embeddings exist\n",
    "if hasattr(topic_model, \"topic_embeddings_\") and topic_model.topic_embeddings_ is not None:\n",
    "    topic_embs = topic_model.topic_embeddings_\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    valid_topics = topic_info[topic_info.Topic != -1][\"Topic\"].tolist()\n",
    "\n",
    "    def nearest_topics_for_text(text, top_k=3):\n",
    "        emb = sbert.encode([text])\n",
    "        sims = cosine_similarity(emb, topic_embs)[0]\n",
    "        # map sims to topic ids (topic_info excluding -1)\n",
    "        pairs = list(zip(valid_topics, sims[:len(valid_topics)]))\n",
    "        pairs = sorted(pairs, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        return pairs\n",
    "else:\n",
    "    print(\"topic_embeddings_ not available. Refit BERTopic with 'calculate_probabilities=True' or compute centroids manually.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
